The corpus directory stores the .txt file used to train the GPT-2 model. This .txt. file can be generated in many ways. One option is to use web scraping utilities, like [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), to gather content from the web. Another option is to use publicly-available literary sources, like [Project Gutenberg](https://www.gutenberg.orgmark). Yet another option is to use APIs provided by whatever information source you are interested in using for training. For example, [Wikipedia provides an API](https://www.mediawiki.org/wiki/API:Get_the_contents_of_a_page) for getting various site content.

Wherever you gather your training dataset, the content should be assembled into a .txt file with all formatting and any special characters (like Emoji) removed. The resulting .txt file is called a "corpus" for finetuning the GPT-2 model. The desired result is to have GPT-2 create text that is consistent with the style of your source material. 
